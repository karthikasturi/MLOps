# Complete Demo and Lab Exercises for Day 2: Environment Setup and MLflow

## Pre-Lab Setup

### Demo: Environment Verification

**Step 1: Anaconda Installation Check**

```bash
# Check if Anaconda is installed
conda --version
# Expected output: conda 23.x.x or higher

# Check available environments
conda env list

# Update conda if needed
conda update conda
```

**Step 2: Python 3.10+ Setup**

```bash
# Create new environment with Python 3.10+
conda create -n mlops-day2 python=3.10 -y

# Activate environment
conda activate mlops-day2

# Verify Python version
python --version
# Should show Python 3.10.x or higher
```

**Step 3: Docker Desktop Verification**

```bash
# Check Docker installation
docker --version
# Expected: Docker version 24.x.x or higher

# Test Docker functionality
docker run hello-world
# Should download and run successfully

# Check Docker Desktop is running
docker ps
# Should show empty list (no error)
```

## Lab Exercise 1: MLflow Installation and Basic Setup (25 minutes)

### Step 1: Install MLflow and Dependencies

```bash
# Ensure you're in the correct environment
conda activate mlops-day2

# Install MLflow and additional packages
pip install mlflow==2.8.1
pip install scikit-learn pandas matplotlib seaborn
pip install flask boto3 psutil

# Verify installation
mlflow --version
# Expected output: mlflow, version 2.8.1
```

### Step 2: Project Structure Setup

```bash
# Navigate to project directory
cd mlops-llm-course

# Create MLflow-specific directories
mkdir -p mlflow-experiments
mkdir -p model-registry
mkdir -p artifacts

# Project structure should now be:
# mlops-llm-course/
# ‚îú‚îÄ‚îÄ data/
# ‚îú‚îÄ‚îÄ models/
# ‚îú‚îÄ‚îÄ notebooks/
# ‚îú‚îÄ‚îÄ logs/
# ‚îú‚îÄ‚îÄ mlflow-experiments/
# ‚îú‚îÄ‚îÄ model-registry/
# ‚îú‚îÄ‚îÄ artifacts/
# ‚îî‚îÄ‚îÄ app.py
```

### Step 3: Start MLflow Tracking Server

```bash
# Method 1: Simple local setup
mlflow ui --host 0.0.0.0 --port 5000

# Method 2: With backend store (recommended)
mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./artifacts \
    --host 0.0.0.0 \
    --port 5000
```

**Verification:**

- Open browser to `http://localhost:5000`
- Should see MLflow UI with empty experiments list
- Keep this terminal running throughout the exercises

### Step 4: Create First MLflow Experiment

Create file: `notebooks/mlflow_basic_setup.ipynb`

```python
mport mlflow
import mlflow.sklearn
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# Set MLflow tracking URI (if using remote server)
mlflow.set_tracking_uri("http://localhost:5000")

# Create or set experiment
experiment_name = "day02_basic_tracking"
mlflow.set_experiment(experiment_name)

print(f"Active experiment: {mlflow.get_experiment_by_name(experiment_name).experiment_id}")
print(f"Tracking URI: {mlflow.get_tracking_uri()}")

# Load data from Day 1
df = pd.read_csv('../data/synthetic_dataset.csv')
X = df.drop('target', axis=1)
y = df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Data loaded successfully!")
print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
```

## Lab Exercise 2: Basic MLflow Tracking

### Step 1: Track Simple Model Training

Add to the same notebook:

```python
# Experiment 1: Random Forest with MLflow tracking
with mlflow.start_run(run_name="random_forest_baseline") as run:
    # Log run info
    print(f"Run ID: {run.info.run_id}")

    # Define model parameters
    n_estimators = 100
    max_depth = 10
    random_state = 42

    # Log parameters
    mlflow.log_param("model_type", "RandomForest")
    mlflow.log_param("n_estimators", n_estimators)
    mlflow.log_param("max_depth", max_depth)
    mlflow.log_param("random_state", random_state)
    mlflow.log_param("test_size", 0.2)

    # Train model
    rf_model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state
    )

    rf_model.fit(X_train, y_train)

    # Make predictions
    y_pred = rf_model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Log metrics
    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("precision", precision)
    mlflow.log_metric("recall", recall)
    mlflow.log_metric("f1_score", f1)

    # Log additional info
    mlflow.log_metric("train_samples", len(X_train))
    mlflow.log_metric("test_samples", len(X_test))

    print(f"Random Forest Results:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    # Save and log model
    mlflow.sklearn.log_model(rf_model, name="model")

    # Create and log confusion matrix plot
    from sklearn.metrics import confusion_matrix

    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Random Forest Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.savefig('confusion_matrix_rf.png')

    # Log the plot as artifact
    mlflow.log_artifact('confusion_matrix_rf.png')
    plt.show()

    # Log feature importance
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    # Save feature importance as CSV
    feature_importance.to_csv('feature_importance_rf.csv', index=False)
    mlflow.log_artifact('feature_importance_rf.csv')

    print("\nTop 5 Important Features:")
    print(feature_importance.head())
```

### Step 2: Compare Multiple Models

Continue in the same notebook:

```python
# Experiment 2: Logistic Regression
with mlflow.start_run(run_name="logistic_regression_baseline") as run:
    print(f"Run ID: {run.info.run_id}")

    # Define parameters
    C = 1.0
    max_iter = 1000
    solver = 'liblinear'

    # Log parameters
    mlflow.log_param("model_type", "LogisticRegression")
    mlflow.log_param("C", C)
    mlflow.log_param("max_iter", max_iter)
    mlflow.log_param("solver", solver)
    mlflow.log_param("random_state", random_state)

    # Train model
    lr_model = LogisticRegression(
        C=C,
        max_iter=max_iter,
        solver=solver,
        random_state=random_state
    )

    lr_model.fit(X_train, y_train)
    y_pred_lr = lr_model.predict(X_test)

    # Calculate and log metrics
    accuracy = accuracy_score(y_test, y_pred_lr)
    precision = precision_score(y_test, y_pred_lr, average='weighted')
    recall = recall_score(y_test, y_pred_lr, average='weighted')
    f1 = f1_score(y_test, y_pred_lr, average='weighted')

    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("precision", precision)
    mlflow.log_metric("recall", recall)
    mlflow.log_metric("f1_score", f1)

    print(f"Logistic Regression Results:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    # Log model
    mlflow.sklearn.log_model(lr_model, "model")

# Experiment 3: Random Forest with hyperparameter tuning
hyperparameters = [
    {"n_estimators": 50, "max_depth": 5},
    {"n_estimators": 100, "max_depth": 10},
    {"n_estimators": 200, "max_depth": 15},
]

for i, params in enumerate(hyperparameters):
    with mlflow.start_run(run_name=f"random_forest_tuned_{i+1}") as run:
        print(f"Run ID: {run.info.run_id}")

        # Log parameters
        mlflow.log_param("model_type", "RandomForest_Tuned")
        mlflow.log_param("n_estimators", params["n_estimators"])
        mlflow.log_param("max_depth", params["max_depth"])
        mlflow.log_param("random_state", random_state)

        # Train model
        rf_tuned = RandomForestClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            random_state=random_state
        )

        rf_tuned.fit(X_train, y_train)
        y_pred_tuned = rf_tuned.predict(X_test)

        # Calculate and log metrics
        accuracy = accuracy_score(y_test, y_pred_tuned)
        precision = precision_score(y_test, y_pred_tuned, average='weighted')
        recall = recall_score(y_test, y_pred_tuned, average='weighted')
        f1 = f1_score(y_test, y_pred_tuned, average='weighted')

        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)

        print(f"RF Tuned {i+1} - n_est:{params['n_estimators']}, max_depth:{params['max_depth']}")
        print(f"  Accuracy: {accuracy:.4f}")

        # Log model
        mlflow.sklearn.log_model(rf_tuned, name="model")

print("\n‚úÖ All experiments completed! Check MLflow UI to compare results.")
```

## Lab Exercise 3: Model Registry and Versioning

### Step 1: Register Best Model

Create file: `notebooks/model_registry_demo.ipynb`

```python
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

# Set tracking URI
mlflow.set_tracking_uri("http://localhost:5000")

# Initialize MLflow client
client = MlflowClient()

# Get experiment
experiment = mlflow.get_experiment_by_name("day02_basic_tracking")
experiment_id = experiment.experiment_id

print(f"Experiment ID: {experiment_id}")

# Get all runs from the experiment
runs = mlflow.search_runs(experiment_ids=[experiment_id])
print(f"Total runs found: {len(runs)}")

# Display run results sorted by accuracy
runs_sorted = runs.sort_values('metrics.accuracy', ascending=False)
print("\nRuns sorted by accuracy:")
print(runs_sorted[['run_id', 'metrics.accuracy', 'params.model_type', 'params.n_estimators']])

# Get the best run
best_run = runs_sorted.iloc[0]
best_run_id = best_run['run_id']
best_accuracy = best_run['metrics.accuracy']

print(f"\nBest run ID: {best_run_id}")
print(f"Best accuracy: {best_accuracy:.4f}")
print(f"Model type: {best_run['params.model_type']}")
```

### Step 2: Register Model to Registry

Continue in the same notebook:

```python
# Register the best model
model_name = "customer_classifier_v2"
model_uri = f"runs:/{best_run_id}/model"

# Register model
registered_model = mlflow.register_model(
    model_uri=model_uri,
    name=model_name
)

print(f"Model registered: {registered_model.name}")
print(f"Model version: {registered_model.version}")

# Add model description
client.update_registered_model(
    name=model_name,
    description="Customer classification model trained on synthetic dataset. Best performing model from Day 2 experiments."
)

# Add version description
client.update_model_version(
    name=model_name,
    version=registered_model.version,
    description=f"Random Forest model with accuracy: {best_accuracy:.4f}"
)

print("‚úÖ Model registered successfully!")

# List all registered models
registered_models = client.search_registered_models()
print(f"\nRegistered models: {len(registered_models)}")
for model in registered_models:
    print(f"  - {model.name}")
```

### Step 3: Model Stage Management

Continue in the same notebook:

```python
# Transition model to Staging
client.set_registered_model_alias(
    name=model_name,
    alias="Staging",
    version=registered_model.version
)

print(f"Model {model_name} v{registered_model.version} moved to Staging")

# Get model version details
model_version = client.get_model_version(
    name=model_name,
    version=registered_model.version
)

print(f"Current stage: {model_version.current_stage}")
print(f"Status: {model_version.status}")

# Register a second model (lower performing) for comparison
second_best_run = runs_sorted.iloc[1]
second_run_id = second_best_run['run_id']
second_accuracy = second_best_run['metrics.accuracy']

model_uri_2 = f"runs:/{second_run_id}/model"
registered_model_2 = mlflow.register_model(
    model_uri=model_uri_2,
    name=model_name  # Same model name, different version
)

print(f"\nSecond model registered as version: {registered_model_2.version}")

# Add description to second version
client.update_model_version(
    name=model_name,
    version=registered_model_2.version,
    description=f"Alternative model with accuracy: {second_accuracy:.4f}"
)

# List all versions of the model
model_versions = client.search_model_versions(f"name='{model_name}'")
print(f"\nModel versions for {model_name}:")
for version in model_versions:
    print(f"  Version {version.version}: {version.current_stage} - {version.description}")

# Promote best model to Production
client.set_registered_model_alias(
    name=model_name,
    alias="Production",
    version=registered_model.version
)

print(f"\n‚úÖ Model {model_name} v{registered_model.version} promoted to Production!")
```

## Lab Exercise 4: Enhanced Flask App with MLflow

### Step 1: Update Flask App with MLflow Integration

Create file: `mlflow_app.py`

```python
from flask import Flask, request, jsonify
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient
import pandas as pd
import numpy as np
import os
from datetime import datetime
import logging

app = Flask(__name__)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MLflow configuration
mlflow.set_tracking_uri("http://localhost:5000")
client = MlflowClient()

class MLflowModelManager:
    def __init__(self, model_name="customer_classifier_v2"):
        self.model_name = model_name
        self.model = None
        self.model_version = None
        self.model_alias = None  # Changed from model_stage
        self.load_production_model()

    def load_production_model(self):
        """Load the latest production model from MLflow registry using aliases"""
        try:
            # Try to get model with 'production' alias
            try:
                model_uri = f"models:/{self.model_name}@production"
                self.model = mlflow.sklearn.load_model(model_uri)

                # Get model version info
                model_versions = client.search_model_versions(f"name='{self.model_name}'")

                # Find the version with production alias
                for version in model_versions:
                    if hasattr(version, 'aliases') and 'production' in (version.aliases or []):
                        self.model_version = version.version
                        self.model_alias = "production"
                        break

                logger.info(f"Loaded production model: {self.model_name} v{self.model_version}")
                return True

            except Exception:
                # Fallback to staging alias
                model_uri = f"models:/{self.model_name}@staging"
                self.model = mlflow.sklearn.load_model(model_uri)

                # Get model version info for staging
                model_versions = client.search_model_versions(f"name='{self.model_name}'")

                for version in model_versions:
                    if hasattr(version, 'aliases') and 'staging' in (version.aliases or []):
                        self.model_version = version.version
                        self.model_alias = "staging"
                        break

                logger.info(f"Loaded staging model: {self.model_name} v{self.model_version}")
                return True

        except Exception as e:
            logger.error(f"Error loading model with aliases: {str(e)}")

            # Final fallback - load latest version without alias
            try:
                model_versions = client.search_model_versions(f"name='{self.model_name}'")
                if model_versions:
                    latest_version = max(model_versions, key=lambda x: int(x.version))
                    model_uri = f"models:/{self.model_name}/{latest_version.version}"
                    self.model = mlflow.sklearn.load_model(model_uri)
                    self.model_version = latest_version.version
                    self.model_alias = "latest"

                    logger.info(f"Loaded latest model: {self.model_name} v{self.model_version}")
                    return True

            except Exception as fallback_error:
                logger.error(f"All model loading attempts failed: {str(fallback_error)}")
                return False

        return False

    def predict(self, features):
        """Make prediction and log to MLflow"""
        if self.model is None:
            raise ValueError("No model loaded")

        # Make prediction
        prediction = self.model.predict([features])[0]
        probabilities = self.model.predict_proba([features])[0]

        # Start MLflow run for prediction logging
        with mlflow.start_run(run_name="prediction_inference"):
            # Log prediction details
            mlflow.log_param("model_name", self.model_name)
            mlflow.log_param("model_version", self.model_version)
            mlflow.log_param("model_alias", self.model_alias)  # Changed from model_stage

            # Log input features (sample)
            for i, feature_val in enumerate(features[:5]):
                mlflow.log_param(f"feature_{i}", feature_val)

            # Log prediction
            mlflow.log_metric("prediction", int(prediction))
            mlflow.log_metric("confidence", float(max(probabilities)))

        return prediction, probabilities

# Initialize model manager
model_manager = MLflowModelManager()

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_manager.model is not None,
        'model_name': model_manager.model_name,
        'model_version': model_manager.model_version,
        'model_alias': model_manager.model_alias,  # Changed from model_stage
        'mlflow_uri': mlflow.get_tracking_uri(),
        'timestamp': datetime.now().isoformat()
    })

@app.route('/predict', methods=['POST'])
def predict():
    try:
        if model_manager.model is None:
            return jsonify({'error': 'No model loaded'}), 500

        # Get JSON data
        data = request.get_json()
        features = data['features']

        # Validate input
        if len(features) != 10:
            return jsonify({'error': 'Expected 10 features'}), 400

        # Make prediction
        prediction, probabilities = model_manager.predict(features)

        response = {
            'prediction': int(prediction),
            'probabilities': probabilities.tolist(),
            'confidence': float(max(probabilities)),
            'model_info': {
                'name': model_manager.model_name,
                'version': model_manager.model_version,
                'alias': model_manager.model_alias  # Changed from stage
            },
            'timestamp': datetime.now().isoformat()
        }

        return jsonify(response)

    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        return jsonify({'error': str(e)}), 400

@app.route('/model/reload', methods=['POST'])
def reload_model():
    """Reload model from registry"""
    try:
        success = model_manager.load_production_model()
        if success:
            return jsonify({
                'message': 'Model reloaded successfully',
                'model_name': model_manager.model_name,
                'model_version': model_manager.model_version,
                'model_alias': model_manager.model_alias  # Changed from model_stage
            })
        else:
            return jsonify({'error': 'Failed to reload model'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/models', methods=['GET'])
def list_models():
    """List all available models in registry with aliases"""
    try:
        registered_models = client.search_registered_models()
        models_info = []

        for model in registered_models:
            versions = client.search_model_versions(f"name='{model.name}'")
            model_info = {
                'name': model.name,
                'description': model.description,
                'versions': []
            }

            for version in versions:
                version_info = {
                    'version': version.version,
                    'status': version.status,
                    'description': version.description,
                    'aliases': version.aliases if hasattr(version, 'aliases') else []  # New field
                }
                model_info['versions'].append(version_info)

            models_info.append(model_info)

        return jsonify({'models': models_info})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    if not model_manager.model:
        logger.warning("No model loaded! Please register a model first.")

    app.run(debug=True, host='0.0.0.0', port=5001)
```

### Step 2: Test Enhanced Flask App

Create file: `notebooks/test_mlflow_app.ipynb`

```python
import requests
import json
import numpy as np

# App URL (note the different port)
BASE_URL = 'http://localhost:5001'

print("Testing Enhanced MLflow Flask App")
print("=" * 40)

# Test 1: Health check
print("1. Health Check")
try:
    response = requests.get(f'{BASE_URL}/health')
    health_data = response.json()
    print(f"‚úÖ Status: {health_data['status']}")
    print(f"‚úÖ Model: {health_data['model_name']} v{health_data['model_version']} ({health_data['model_alias']})")
    print(f"‚úÖ MLflow URI: {health_data['mlflow_uri']}")
except Exception as e:
    print(f"‚ùå Health check failed: {e}")

print("\n" + "="*40)

# Test 2: List available models
print("2. Available Models")
try:
    response = requests.get(f'{BASE_URL}/models')
    models_data = response.json()
    print(f"‚úÖ Found {len(models_data['models'])} registered models:")

    for model in models_data['models']:
        print(f"  üì¶ {model['name']}")
        for version in model['versions']:
            print(f"    - v{version['version']}: {version['aliases']} ({version['status']})")
except Exception as e:
    print(f"‚ùå Model listing failed: {e}")

print("\n" + "="*40)

# Test 3: Make predictions
print("3. Predictions")
try:
    # Test with different feature sets
    test_cases = [
        {"name": "Test Case 1", "features": np.random.randn(10).tolist()},
        {"name": "Test Case 2", "features": np.random.randn(10).tolist()},
        {"name": "Test Case 3", "features": [0.5, -0.2, 1.1, 0.8, -0.5, 0.3, -0.1, 0.9, 0.2, -0.3]}
    ]

    for test_case in test_cases:
        prediction_data = {"features": test_case["features"]}

        response = requests.post(
            f'{BASE_URL}/predict',
            json=prediction_data,
            headers={'Content-Type': 'application/json'}
        )

        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ {test_case['name']}:")
            print(f"   Prediction: Class {result['prediction']}")
            print(f"   Confidence: {result['confidence']:.4f}")
            print(f"   Model: {result['model_info']['name']} v{result['model_info']['version']}")
        else:
            print(f"‚ùå {test_case['name']} failed: {response.text}")

except Exception as e:
    print(f"‚ùå Prediction test failed: {e}")

print("\n" + "="*40)

# Test 4: Model reload
print("4. Model Reload")
try:
    response = requests.post(f'{BASE_URL}/model/reload')
    if response.status_code == 200:
        reload_data = response.json()
        print(f"‚úÖ Model reloaded: {reload_data['model_name']} v{reload_data['model_version']}")
    else:
        print(f"‚ö†Ô∏è Reload response: {response.text}")
except Exception as e:
    print(f"‚ùå Model reload failed: {e}")

print("\n" + "="*40)
print("üéâ Testing completed!")
print("\nüìä Check MLflow UI for prediction logs:")
print("   http://localhost:5000")
```

## Lab Exercise 5: Dataset Versioning and Tracking

### Step 1: Implement Dataset Versioning

Create file: `notebooks/dataset_versioning.ipynb`

```python
import mlflow
import mlflow.data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import hashlib
import json
from datetime import datetime

# Set MLflow tracking
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("dataset_versioning_demo")

def calculate_dataset_hash(df):
    """Calculate hash for dataset versioning"""
    return hashlib.md5(pd.util.hash_pandas_object(df, index=True).values).hexdigest()

def log_dataset_profile(df, dataset_name):
    """Log comprehensive dataset profile"""
    profile = {
        'name': dataset_name,
        'shape': df.shape,
        'columns': df.columns.tolist(),
        'dtypes': df.dtypes.astype(str).to_dict(),
        'missing_values': df.isnull().sum().to_dict(),
        'memory_usage': df.memory_usage(deep=True).sum(),
        'hash': calculate_dataset_hash(df)
    }

    # Numeric column statistics
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        profile['numeric_stats'] = df[numeric_cols].describe().to_dict()

    return profile

# Step 1: Load and profile original dataset
print("üìä Dataset Versioning Demo")
print("="*50)

# Load original data
df_original = pd.read_csv('../data/synthetic_dataset.csv')
print(f"Original dataset shape: {df_original.shape}")

# Version 1: Original dataset
with mlflow.start_run(run_name="dataset_v1_original"):
    # Create MLflow dataset
    dataset_v1 = mlflow.data.from_pandas(
        df_original,
        source="../data/synthetic_dataset.csv",
        name="synthetic_dataset_v1"
    )

    # Log dataset
    mlflow.log_input(dataset_v1, context="original")

    # Log dataset profile
    profile_v1 = log_dataset_profile(df_original, "synthetic_dataset_v1")

    # Log profile as parameters and metrics
    mlflow.log_param("dataset_name", profile_v1['name'])
    mlflow.log_param("dataset_hash", profile_v1['hash'])
    mlflow.log_param("num_rows", profile_v1['shape'][0])
    mlflow.log_param("num_columns", profile_v1['shape'][1])
    mlflow.log_metric("memory_usage_bytes", profile_v1['memory_usage'])
    mlflow.log_metric("missing_values_total", sum(profile_v1['missing_values'].values()))

    # Save profile as artifact
    with open('dataset_profile_v1.json', 'w') as f:
        json.dump(profile_v1, f, indent=2, default=str)
    mlflow.log_artifact('dataset_profile_v1.json')

    print(f"‚úÖ Dataset v1 logged - Hash: {profile_v1['hash'][:8]}...")

# Version 2: Standardized dataset
print("\nüìà Creating standardized version...")

# Apply standardization
X = df_original.drop('target', axis=1)
y = df_original['target']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create standardized dataframe
df_standardized = pd.DataFrame(X_scaled, columns=X.columns)
df_standardized['target'] = y

with mlflow.start_run(run_name="dataset_v2_standardized"):
    # Create MLflow dataset
    dataset_v2 = mlflow.data.from_pandas(
        df_standardized,
        source="data_preprocessing_pipeline",
        name="synthetic_dataset_v2_standardized"
    )

    # Log dataset
    mlflow.log_input(dataset_v2, context="preprocessed")

    # Log preprocessing parameters
    mlflow.log_param("preprocessing", "StandardScaler")
    mlflow.log_param("scaler_mean", scaler.mean_.tolist())
    mlflow.log_param("scaler_scale", scaler.scale_.tolist())

    # Log dataset profile
    profile_v2 = log_dataset_profile(df_standardized, "synthetic_dataset_v2_standardized")

    mlflow.log_param("dataset_name", profile_v2['name'])
    mlflow.log_param("dataset_hash", profile_v2['hash'])
    mlflow.log_param("num_rows", profile_v2['shape'][0])
    mlflow.log_param("num_columns", profile_v2['shape'][1])
    mlflow.log_metric("memory_usage_bytes", profile_v2['memory_usage'])

    # Save standardized dataset
    df_standardized.to_csv('synthetic_dataset_v2_standardized.csv', index=False)
    mlflow.log_artifact('synthetic_dataset_v2_standardized.csv')

    # Save profile
    with open('dataset_profile_v2.json', 'w') as f:
        json.dump(profile_v2, f, indent=2, default=str)
    mlflow.log_artifact('dataset_profile_v2.json')

    # Save scaler
    import joblib
    joblib.dump(scaler, 'scaler_v2.pkl')
    mlflow.log_artifact('scaler_v2.pkl')

    print(f"‚úÖ Dataset v2 logged - Hash: {profile_v2['hash'][:8]}...")

# Version 3: Feature engineered dataset
print("\nüîß Creating feature engineered version...")

# Add feature engineering
df_engineered = df_original.copy()

# Add polynomial features for first 3 columns
for i in range(3):
    col_name = f'feature_{i}'
    df_engineered[f'{col_name}_squared'] = df_engineered[col_name] ** 2

    # Interaction features
    for j in range(i+1, 3):
        other_col = f'feature_{j}'
        df_engineered[f'{col_name}_x_{other_col}'] = df_engineered[col_name] * df_engineered[other_col]

# Add statistical features
df_engineered['feature_sum'] = df_engineered[[f'feature_{i}' for i in range(10)]].sum(axis=1)
df_engineered['feature_mean'] = df_engineered[[f'feature_{i}' for i in range(10)]].mean(axis=1)
df_engineered['feature_std'] = df_engineered[[f'feature_{i}' for i in range(10)]].std(axis=1)

with mlflow.start_run(run_name="dataset_v3_feature_engineered"):
    # Create MLflow dataset
    dataset_v3 = mlflow.data.from_pandas(
        df_engineered,
        source="feature_engineering_pipeline",
        name="synthetic_dataset_v3_engineered"
    )

    # Log dataset
    mlflow.log_input(dataset_v3, context="feature_engineered")

    # Log feature engineering details
    mlflow.log_param("feature_engineering", "polynomial_and_statistical")
    mlflow.log_param("polynomial_degree", 2)
    mlflow.log_param("interaction_features", True)
    mlflow.log_param("statistical_features", ["sum", "mean", "std"])

    # Log dataset profile
    profile_v3 = log_dataset_profile(df_engineered, "synthetic_dataset_v3_engineered")

    mlflow.log_param("dataset_name", profile_v3['name'])
    mlflow.log_param("dataset_hash", profile_v3['hash'])
    mlflow.log_param("num_rows", profile_v3['shape'][0])
    mlflow.log_param("num_columns", profile_v3['shape'][1])
    mlflow.log_metric("memory_usage_bytes", profile_v3['memory_usage'])

    # Save engineered dataset
    df_engineered.to_csv('synthetic_dataset_v3_engineered.csv', index=False)
    mlflow.log_artifact('synthetic_dataset_v3_engineered.csv')

    # Save profile
    with open('dataset_profile_v3.json', 'w') as f:
        json.dump(profile_v3, f, indent=2, default=str)
    mlflow.log_artifact('dataset_profile_v3.json')

    print(f"‚úÖ Dataset v3 logged - Hash: {profile_v3['hash'][:8]}...")

print(f"\nüìä Dataset Summary:")
print(f"   v1 (Original): {profile_v1['shape'][1]} features, {profile_v1['hash'][:8]}...")
print(f"   v2 (Standardized): {profile_v2['shape'][1]} features, {profile_v2['hash'][:8]}...")
print(f"   v3 (Engineered): {profile_v3['shape'][1]} features, {profile_v3['hash'][:8]}...")
```

### Step 2: Data Lineage Tracking

Continue in the same notebook:

```python
# Step 2: Demonstrate data lineage tracking
print("\nüîÑ Data Lineage Tracking")
print("="*30)

def create_lineage_map():
    """Create data lineage visualization"""
    lineage = {
        'datasets': [
            {
                'name': 'synthetic_dataset_v1',
                'hash': profile_v1['hash'][:8],
                'source': 'original_data_generation',
                'transformations': []
            },
            {
                'name': 'synthetic_dataset_v2',
                'hash': profile_v2['hash'][:8],
                'source': 'synthetic_dataset_v1',
                'transformations': ['StandardScaler']
            },
            {
                'name': 'synthetic_dataset_v3',
                'hash': profile_v3['hash'][:8],
                'source': 'synthetic_dataset_v1',
                'transformations': ['polynomial_features', 'interaction_features', 'statistical_features']
            }
        ],
        'lineage_map': {
            'v1': 'Original ‚Üí v1',
            'v2': 'v1 ‚Üí StandardScaler ‚Üí v2',
            'v3': 'v1 ‚Üí FeatureEngineering ‚Üí v3'
        }
    }
    return lineage

# Create and log lineage
lineage_info = create_lineage_map()

# Log lineage information in a final run
with mlflow.start_run(run_name="data_lineage_summary"):
    mlflow.log_param("total_dataset_versions", 3)
    mlflow.log_param("lineage_tracked", True)

    # Save lineage map
    with open('data_lineage.json', 'w') as f:
        json.dump(lineage_info, f, indent=2)
    mlflow.log_artifact('data_lineage.json')

    print("‚úÖ Data lineage tracked and saved")

print("\nüéØ Dataset versioning completed!")
print("   Check MLflow UI for complete dataset tracking")
print("   All dataset versions are now traceable and reproducible")
```

## Integration Test and Final Verification

Create file: `notebooks/day2_integration_test.ipynb`

```python
import requests
import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd

print("üß™ Day 2 Integration Test")
print("="*50)

# Test 1: MLflow Server Status
print("1. MLflow Server Status")
try:
    mlflow.set_tracking_uri("http://localhost:5000")
    client = MlflowClient()

    # List experiments
    experiments = client.search_experiments()
    print(f"‚úÖ MLflow server running - {len(experiments)} experiments found")

    for exp in experiments:
        runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
        print(f"   üìÅ {exp.name}: {len(runs)} runs")

except Exception as e:
    print(f"‚ùå MLflow server issue: {e}")

print("\n" + "="*50)

# Test 2: Model Registry Status
print("2. Model Registry Status")
try:
    registered_models = client.search_registered_models()
    print(f"‚úÖ Model registry active - {len(registered_models)} models registered")

    for model in registered_models:
        versions = client.search_model_versions(f"name='{model.name}'")
        production_versions = [v for v in versions if v.current_stage == "Production"]
        staging_versions = [v for v in versions if v.current_stage == "Staging"]

        print(f"   üè∑Ô∏è {model.name}:")
        print(f"      Production: {len(production_versions)} versions")
        print(f"      Staging: {len(staging_versions)} versions")

except Exception as e:
    print(f"‚ùå Model registry issue: {e}")

print("\n" + "="*50)

# Test 3: Enhanced Flask App Status
print("3. Enhanced Flask App Status")
try:
    health_response = requests.get('http://localhost:5001/health')
    if health_response.status_code == 200:
        health_data = health_response.json()
        print(f"‚úÖ Flask app running on port 5001")
        print(f"   Model loaded: {health_data['model_loaded']}")
        print(f"   Model: {health_data['model_name']} v{health_data['model_version']}")
        print(f"   Stage: {health_data['model_stage']}")

        # Test prediction
        test_features = [0.1, -0.2, 0.5, 0.8, -0.1, 0.3, 0.7, -0.4, 0.2, 0.6]
        pred_response = requests.post(
            'http://localhost:5001/predict',
            json={'features': test_features},
            headers={'Content-Type': 'application/json'}
        )

        if pred_response.status_code == 200:
            pred_data = pred_response.json()
            print(f"   Prediction test: ‚úÖ Class {pred_data['prediction']}")
            print(f"   Confidence: {pred_data['confidence']:.4f}")
        else:
            print(f"   Prediction test: ‚ùå {pred_response.text}")

    else:
        print(f"‚ùå Flask app not responding: {health_response.status_code}")

except Exception as e:
    print(f"‚ùå Flask app connection failed: {e}")

print("\n" + "="*50)

# Test 4: Dataset Versioning Check
print("4. Dataset Versioning Check")
try:
    # Check for dataset experiment
    dataset_exp = None
    for exp in experiments:
        if exp.name == "dataset_versioning_demo":
            dataset_exp = exp
            break

    if dataset_exp:
        dataset_runs = mlflow.search_runs(experiment_ids=[dataset_exp.experiment_id])
        print(f"‚úÖ Dataset versioning active - {len(dataset_runs)} dataset versions")

        # Check for different dataset versions
        dataset_versions = dataset_runs['params.dataset_name'].dropna().unique()
        print(f"   Tracked versions: {len(dataset_versions)}")
        for version in dataset_versions:
            print(f"      - {version}")

    else:
        print("‚ö†Ô∏è Dataset versioning experiment not found")

except Exception as e:
    print(f"‚ùå Dataset versioning check failed: {e}")

print("\n" + "="*50)

# Test 5: End-to-End Workflow
print("5. End-to-End Workflow Test")
try:
    # Create a mini experiment to test complete workflow
    mlflow.set_experiment("day2_integration_test")

    with mlflow.start_run(run_name="integration_test_model"):
        # Load data
        df = pd.read_csv('../data/synthetic_dataset.csv')

        # Log dataset
        dataset = mlflow.data.from_pandas(df, source="../data/synthetic_dataset.csv")
        mlflow.log_input(dataset, context="integration_test")

        # Quick model training
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score

        X = df.drop('target', axis=1)
        y = df['target']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train and log model
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X_train, y_train)

        accuracy = accuracy_score(y_test, model.predict(X_test))

        mlflow.log_param("n_estimators", 10)
        mlflow.log_metric("accuracy", accuracy)
        mlflow.sklearn.log_model(model, "integration_test_model")

        print(f"‚úÖ End-to-end test completed")
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   Model logged successfully")

except Exception as e:
    print(f"‚ùå End-to-end test failed: {e}")

print("\n" + "="*50)

# Final Summary
print("üìä Day 2 Integration Summary")
print("="*30)

summary = {
    'mlflow_server': '‚úÖ Running',
    'model_registry': '‚úÖ Active',
    'flask_app': '‚úÖ Running',
    'dataset_versioning': '‚úÖ Implemented',
    'end_to_end_workflow': '‚úÖ Tested'
}

for component, status in summary.items():
    print(f"{component.replace('_', ' ').title()}: {status}")

print("\nüéâ Day 2 setup complete!")
print("üöÄ Ready for Day 3: Model Validation and Deployment")

print("\nüìã Quick Access URLs:")
print("   MLflow UI: http://localhost:5000")
print("   Flask API: http://localhost:5001")
print("   Health Check: http://localhost:5001/health")
print("   Model Info: http://localhost:5001/models")
```

## Instructions for Running Day 2 Labs

### Pre-Lab Checklist

1. **Ensure Day 1 environment is ready**
2. **Install Anaconda/Docker if not done**
3. **Have project directory structure ready**

### Execution Order

1. **Start MLflow server**: `mlflow ui --host 0.0.0.0 --port 5000` (keep running)
2. **Run Exercise 1**: MLflow installation and basic setup (25 min)
3. **Run Exercise 2**: Basic MLflow tracking (35 min)
4. **Run Exercise 3**: Model registry and versioning (30 min)
5. **Run Exercise 4**: Enhanced Flask app (25 min)
6. **Start enhanced app**: `python mlflow_app.py` (keep running)
7. **Run Exercise 5**: Dataset versioning (20 min)
8. **Run Integration test**: Final verification (15 min)

### Expected Outcomes

- ‚úÖ Complete MLOps environment with MLflow
- ‚úÖ Model tracking and versioning system
- ‚úÖ Production-ready model registry
- ‚úÖ Enhanced API with MLflow integration
- ‚úÖ Dataset versioning and lineage tracking
- ‚úÖ End-to-end MLOps workflow

### Troubleshooting Tips

- **Port conflicts**: Use different ports if 5000/5001 are occupied
- **MLflow UI issues**: Check if server is running and accessible
- **Model loading errors**: Ensure models are registered correctly
- **Docker issues**: Verify Docker Desktop is running
- **Import errors**: Check virtual environment activation
